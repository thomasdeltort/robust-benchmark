# This is an example configuration file that contains most useful parameter settings.
# model:
  # name: CNNA_CIFAR10_1_LIP_GNP  # This model is defined in model_defs.py. Add your own model definitions there.
  # The model is from "Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds (Huang et al., NeurIPS 2021)"
  # wget https://github.com/yjhuangcd/local-lipschitz/raw/main/pretrained/relu/cifar_4C3F_best.pth
  # path: /home/aws_install/robustess_project/Robust_Benchmark/models/cifar10_CNNA_CIFAR10_1_LIP_GNP_temp1.0_bs256_trainacc0.0_valacc0.6.pth
data:
  # dataset: CIFAR_SDP  # Dataset name. This is just the standard CIFAR-10 test set defined in the "load_verification_dataset()" function in utils.py
  mean: [0.4914, 0.4822, 0.4465]  # Mean for normalization.
  std: [0.225, 0.225, 0.225]  # Std for normalization.
  start: 0  # First example to verify in dataset.
  end: 200 # Last example to verify in dataset.
specification:
  # type: lp
  norm: .inf  # Linf norm (can also be 2 or 1).
  # epsilon: 0.1411764705882353  # epsilon=36./255.
attack:  # Currently attack is only implemented for Linf norm. Skip for L2 norm.
  # pgd_order: skip
  pgd_restarts: 10
solver:
  batch_size: 1024  # Number of subdomains to compute in parallel in bound solver. Decrease if you run out of memory.
  alpha-crown:
    iteration: 200   # Number of iterations for alpha-CROWN optimization. Alpha-CROWN is used to compute all intermediate layer bounds before branch and bound starts.
    lr_alpha: 0.1    # Learning rate for alpha in alpha-CROWN. The default (0.1) is typically ok.
  beta-crown:
    lr_alpha: 0.01  # Learning rate for optimizing the alpha parameters, the default (0.01) is typically ok, but you can try to tune this parameter to get better lower bound.
    lr_beta: 0.05  # Learning rate for optimizing the beta parameters, the default (0.05) is typically ok, but you can try to tune this parameter to get better lower bound.
    iteration: 50  # Number of iterations for beta-CROWN optimization. 20 is often sufficient, 50 or 100 can also be used.
bab:
  timeout: 120  # Timeout threshold for branch and bound. Increase for verifying more points.
  branching:  # Parameters for branching heuristics.
    reduceop: min  # Reduction function for the branching heuristic scores, min or max. Using max can be better on some models.
    method: kfsb  # babsr is fast but less accurate; fsb is slow but most accurate; kfsb is usually a balance.
    candidates: 3  # Number of candidates to consider in fsb and kfsb. More leads to slower but better branching. 3 is typically good enough.
